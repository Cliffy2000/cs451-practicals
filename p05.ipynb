{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fluid-blocking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, json\n",
    "from shared import dataset_local_path, TODO\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "medical-division",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class JustWikiPage:\n",
    "    title: str\n",
    "    wiki_id: str\n",
    "    body: str\n",
    "\n",
    "\n",
    "# Load our pages into this pages list.\n",
    "pages: List[JustWikiPage] = []\n",
    "with gzip.open(dataset_local_path(\"tiny-wiki.jsonl.gz\"), \"rt\") as fp:\n",
    "    for line in fp:\n",
    "        entry = json.loads(line)\n",
    "        pages.append(JustWikiPage(**entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "electric-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class JustWikiLabel:\n",
    "    wiki_id: str\n",
    "    is_literary: bool\n",
    "\n",
    "\n",
    "# Load our judgments/labels/truths/ys into this labels list:\n",
    "labels: List[JustWikiLabel] = []\n",
    "with open(dataset_local_path(\"tiny-wiki-labels.jsonl\")) as fp:\n",
    "    for line in fp:\n",
    "        entry = json.loads(line)\n",
    "        labels.append(\n",
    "            JustWikiLabel(wiki_id=entry[\"wiki_id\"], is_literary=entry[\"truth_value\"])\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class JoinedWikiData:\n",
    "    wiki_id: str\n",
    "    is_literary: bool\n",
    "    title: str\n",
    "    body: str\n",
    "\n",
    "    def __str__(self):\n",
    "        print('wiki_id:', self.wiki_id)\n",
    "        print('is_literary:', self.is_literary)\n",
    "        print('title:', self.title)\n",
    "        print('body:', self.body)\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "structural-times",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data: Dict[str, JoinedWikiData] = {}\n",
    "pages.sort(key = lambda x: x.wiki_id)\n",
    "labels.sort(key = lambda x: x.wiki_id)\n",
    "\n",
    "for i in range(len(pages)):\n",
    "    joined_data[i] = JoinedWikiData(wiki_id = pages[i].wiki_id,\n",
    "                                    is_literary = labels[i].is_literary,\n",
    "                                    title = pages[i].title,\n",
    "                                    body = pages[i].body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "orange-flush",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_id: enwiki:%C3%81ngeles%20sin%20para%C3%ADso\n",
      "is_literary: False\n",
      "title: Ángeles sin paraíso\n",
      "body: Ángeles sin paraíso (English title: Angels without paradise) is a Mexican Children's telenovela produced by Pedro Damián for Televisa in 1992. This telenovela is remembered by the public as one of the first children's Mexican telenovela.\n",
      "\n",
      "Anahí and Felipe Colombo starred as child protagonists, while Patricia Bernal starred as main antagonist.\n",
      "\n",
      "Category:1992 telenovelasCategory:Mexican telenovelasCategory:1992 Mexican television series debutsCategory:1993 Mexican television series endingsCategory:Spanish-language telenovelasCategory:Television shows set in MexicoCategory:Televisa telenovelasCategory:Children's telenovelas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(joined_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "answering-gossip",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to Learn!\n",
      "SGDClassifier:\n",
      "\tVali-Acc: 0.858\n",
      "\tVali-AUC: 0.898\n",
      "Perceptron:\n",
      "\tVali-Acc: 0.827\n",
      "\tVali-AUC: 0.877\n",
      "LogisticRegression:\n",
      "\tVali-Acc: 0.802\n",
      "\tVali-AUC: 0.9\n",
      "DTree:\n",
      "\tVali-Acc: 0.747\n",
      "\tVali-AUC: 0.719\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make sure it is solved correctly!\n",
    "assert len(joined_data) == len(pages)\n",
    "assert len(joined_data) == len(labels)\n",
    "# Make sure it has *some* positive labels!\n",
    "assert sum([1 for d in joined_data.values() if d.is_literary]) > 0\n",
    "# Make sure it has *some* negative labels!\n",
    "assert sum([1 for d in joined_data.values() if not d.is_literary]) > 0\n",
    "\n",
    "# Construct our ML problem:\n",
    "ys = []\n",
    "examples = []\n",
    "for wiki_data in joined_data.values():\n",
    "    ys.append(wiki_data.is_literary)\n",
    "    examples.append(wiki_data.body)\n",
    "\n",
    "## We're actually going to split before converting to features now...\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "RANDOM_SEED = 1234\n",
    "\n",
    "## split off train/validate (tv) pieces.\n",
    "ex_tv, ex_test, y_tv, y_test = train_test_split(\n",
    "    examples,\n",
    "    ys,\n",
    "    train_size=0.75,\n",
    "    shuffle=True,\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "# split off train, validate from (tv) pieces.\n",
    "ex_train, ex_vali, y_train, y_vali = train_test_split(\n",
    "    ex_tv, y_tv, train_size=0.66, shuffle=True, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "## Convert to features, train simple model (TFIDF will be explained eventually.)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Only learn columns for words in the training data, to be fair.\n",
    "word_to_column = TfidfVectorizer(\n",
    "    strip_accents=\"unicode\", lowercase=True, stop_words=\"english\", max_df=0.5\n",
    ")\n",
    "word_to_column.fit(ex_train)\n",
    "\n",
    "# Test words should surprise us, actually!\n",
    "X_train = word_to_column.transform(ex_train)\n",
    "X_vali = word_to_column.transform(ex_vali)\n",
    "X_test = word_to_column.transform(ex_test)\n",
    "\n",
    "\n",
    "print(\"Ready to Learn!\")\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "models = {\n",
    "    \"SGDClassifier\": SGDClassifier(),\n",
    "    \"Perceptron\": Perceptron(),\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"DTree\": DecisionTreeClassifier(),\n",
    "}\n",
    "\n",
    "for name, m in models.items():\n",
    "    m.fit(X_train, y_train)\n",
    "    print(\"{}:\".format(name))\n",
    "    print(\"\\tVali-Acc: {:.3}\".format(m.score(X_vali, y_vali)))\n",
    "    if hasattr(m, \"decision_function\"):\n",
    "        scores = m.decision_function(X_vali)\n",
    "    else:\n",
    "        scores = m.predict_proba(X_vali)[:, 1]\n",
    "    print(\"\\tVali-AUC: {:.3}\".format(roc_auc_score(y_score=scores, y_true=y_vali)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-growing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
